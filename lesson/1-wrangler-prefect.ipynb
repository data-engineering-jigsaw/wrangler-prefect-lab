{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8210ee6f-4522-4272-bd66-d98d0faafa34",
   "metadata": {},
   "source": [
    "# Wrangler Prefect Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3cf33-0715-4e29-a90f-02fcb6b735cc",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7328009-a6d6-445b-9361-3b09a1e04fb0",
   "metadata": {},
   "source": [
    "In this lesson, we'll move through the wrangler prefect lab.  The lab will use the texas drink receipts api. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce33d3-3785-4fe8-b0be-b30d4acba6a5",
   "metadata": {},
   "source": [
    "### Project Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce7172-c1cc-4176-b3e9-af51bb067a2c",
   "metadata": {},
   "source": [
    "Before moving on, let's talk through why this topic can make for a good project.  We can use alcoholic receipts as a proxy for revenue of a company, and here we can get insights on a per store basis, that we cannot get from a company report.  \n",
    "\n",
    "For example, in the `etl/tx_drinks/seed` file, you can see that we chose restaurants that are publicly listed.  The last set of restaurants are different Applebees chains.  We could imagine a project showing the following.\n",
    "\n",
    "* Overview analysis\n",
    "1. Showing a relationship between drink revenue and overall revenue\n",
    "2. A dashboard with an overall map display of revenue per store\n",
    "3. Then detail  about an individual store, as we click on a store\n",
    "    * Eg. Revenue per month, monthly sales in previous year vs current\n",
    "    \n",
    "* Deeper Analysis\n",
    "And then if given more time, we could explore location based metrics.\n",
    "1. Does income of region predict revenue?\n",
    "2. How about population size?\n",
    "3. Or sales of nearby competitors (eg. Chilis)\n",
    "4. Outliers - Are there over or underperformers that we should look more deeply at?\n",
    "\n",
    "Stats like this would allow us to (1) justify where to open/close retail locations of similar stores (2) assess performance to dig deeper and learn about a store's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b381e-d781-4963-ae22-410d59cf1bc3",
   "metadata": {},
   "source": [
    "### Getting setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127b569-9b92-484e-b8dc-be6b72595cf2",
   "metadata": {},
   "source": [
    "To be able to run the code, we should setup our virtual environment, and install the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375fe28c-20e3-47a9-82ab-dfb38a037eb3",
   "metadata": {},
   "source": [
    "```bash\n",
    "python3 python -m venv ./venv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578d67df-f433-4bd1-836c-38e1190f19c2",
   "metadata": {},
   "source": [
    "```bash\n",
    "source venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11d87d-af4a-48ca-9572-2929aa1f7b84",
   "metadata": {},
   "source": [
    "Now that the environment is activated, we can install the packages in the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad06cf-8e5c-47ab-ada2-63cc48123ec2",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip3 install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a6128-90c4-4bc5-b5e1-aec163b06652",
   "metadata": {},
   "source": [
    "If you look at the `requirements.txt` file, you can see that we installed the following packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fdb921-d4cc-4d6d-a75f-6d0face42495",
   "metadata": {},
   "source": [
    "```\n",
    "prefect\n",
    "requests\n",
    "pandas\n",
    "awswrangler\n",
    "python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582345ab-2dc7-4856-a2df-e0f3d9b34e50",
   "metadata": {},
   "source": [
    "And then we should assign the `PYTHONPATH` to the path of our `revenue_tracker` folder.  This will mean that as we import files, it will always relative to the specified revenue_tracker.  \n",
    "\n",
    "In my environment, I set this by placing the following into bash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d84771-7422-4e3d-ba43-2e3629c4c733",
   "metadata": {},
   "source": [
    "```bash\n",
    "export PYTHONPATH=/Users/jeffreykatz/Documents/jigsaw/curriculum/1-career-services/prefect-lessons/9-aws-wrangler-lab/revenue_tracker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92447cbb-df60-4b14-b126-312c76899892",
   "metadata": {},
   "source": [
    "But you should navigate to your `revenue_tracker` folder, and type `pwd` to find the absolute path to your folder.  \n",
    "\n",
    "Now, when in our code we have something like:\n",
    "\n",
    "```python\n",
    "import settings\n",
    "```\n",
    "\n",
    "It will look for the module in the `revenue_tracker` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74fbaef-8104-43f1-b082-fd6850a6a1eb",
   "metadata": {},
   "source": [
    "From there, we should set the environmental variables, which you can see in the `.env` file.  As you can see, we need a specified bucket folder to store our data, as well as a name for our `glue_db` (eg. `revenue_tracker` is fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb78f8-dfa4-4910-a50a-60c916a59f08",
   "metadata": {},
   "source": [
    "### Kicking off the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d2946-b8e0-4eb8-8d80-01bb84736a00",
   "metadata": {},
   "source": [
    "A good place to start with the project is the `console.py` file.  At the bottom, you can find some useful functions to try out.\n",
    "\n",
    "* `find_all_receipts`\n",
    "    * This makes a request to the API to retrieve our receipt records.  Take a look at the function to see how it works, and then call the function (you can pass through the name specified in the console: `name = \"THE HOUSTON CHEESECAKE FACTORY CORPORATION\"`.  \n",
    "    \n",
    "    \n",
    "* `receipts_adapter.coerce_df`\n",
    "    * This function is tricky.  The idea is to use pandas to convert as many columns as possible into either a numeric or datetime column.  We want to coerce our dataframe before we store the data in s3.  This way, when we save the data, and eventually crawl the data with athena, athena will have those numeric/datetime datatypes.  Notice that we return the dataframe if the dataframe is empty, which may occur if we do not get records back from the api.\n",
    "\n",
    "> So before moving on, give those two methods a shot.  Run the console, and call the `find_all_receipts` and `coerce_df` functions.  Then use the `dtypes` property to confirm that we have more numeric data after passing our dataframe to the `coerce_df` function. \n",
    "\n",
    "\n",
    "* `aws_utils.write_to_s3`\n",
    "    * this is in the `aws_utils` as there are multiple times that we may want to write to s3, and we wanted to avoid repeating the code.  Take a look at the function.  Notice that we are specifying a partition column, and a default mode of `append` to the dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968fa64d-1318-498c-97ec-c8a03f9182b5",
   "metadata": {},
   "source": [
    "#### Next steps\n",
    "\n",
    "* `find_and_coerce`\n",
    "\n",
    "So this is your first task. In the run.py file, take a look at the `find_and_coerce` function so that it given a restaurant name it finds the receipts, and then coerces the resulting dataframe (you can use the functions above).\n",
    "\n",
    "* Update the `find` method\n",
    "    * One thing to note is that by default, the api only returns 1000 records.  So to seed our database, we would [change the limit](https://dev.socrata.com/docs/paging.html) to the max of `50_000` records.  This would give us an initial dataset, and we could schedule our prefect code to repeatedly update this (more below).\n",
    "\n",
    "* Seed the bucket\n",
    "\n",
    "Then use the `find_and_coerce` function to return the coerced dataframe.  And use our `write_to_s3` function to write our data to s3.  **Note**: Before you do so, you'll first need to create a bucket to store your data, and fill in the correct information in the `.env` file.\n",
    "\n",
    "* From there go to the s3 bucket, and confirm that the data was written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e7922-c0a2-41fc-96e5-0e344d13451d",
   "metadata": {},
   "source": [
    "* Add a function called `read_from_s3` that will use wrangler to read from the bucket specified in your `.env` file.  Import this in your `console.py`, and confirm that it does read back the data.  If you call `.dtypes` on the dataframe, you should confirm that many of the columns were retrieved as numeric.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b689a308-6980-4ed5-9f0b-5819f35929e6",
   "metadata": {},
   "source": [
    "### Setting up Glue/Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37845d9-d172-4c61-b26f-779b9dbc7151",
   "metadata": {},
   "source": [
    "So after we have seeded our data lake with data, the next step is to tell Glue to crawl it so that we can query the data with a query engine like Athena.\n",
    "\n",
    "For this, we can continue on at the bottom of the `console.py` file.  \n",
    "\n",
    "* `athena.migrations.create_db`\n",
    "    * This creates a database in glue.\n",
    "    \n",
    "* `athena.migrations.crawl_dataset(\"receipts\")`\n",
    "    * This creates a new table in our database called `receipts`, sets the specified datatypes, and seeds with our data.\n",
    "    \n",
    "* `display_schema(\"receipts\")`\n",
    "    * Call display schema to confirm the data is of the correct type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c97a5-fd3f-4bd5-af38-d78f21bfd1ce",
   "metadata": {},
   "source": [
    "Still if we run those two functions, we should see our database seeded with some initial data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a01e3-7985-4752-b3cf-dfd9b42c73d9",
   "metadata": {},
   "source": [
    "* `athena.queries.read_query`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702fe6ec-7cde-459b-ba17-40230e050a7c",
   "metadata": {},
   "source": [
    "From there, we can query our dataset with athena with our `read_query(query)` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a558b-db88-4b2e-aebb-5fa554f8db67",
   "metadata": {},
   "source": [
    "### Seeding a list of llcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2fff2-7647-451b-b38b-170d08a57516",
   "metadata": {},
   "source": [
    "Eventually, we would like this to work for a list of llcs.  And we want this to be a common list of llcs regardless of whose computer is running this.\n",
    "\n",
    "So in the `console.py` file you can see that we call `write_llcs`, which writes to a parquet file in S3, a list of llc names defined in `seed/restaurant_llcs.py`.  Then we can read them and get back a list of llcs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4577f-8c2b-43de-8ded-e1513c6f3128",
   "metadata": {},
   "source": [
    "### Our Run file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2ab12-5f62-414b-8b4b-991fd067154e",
   "metadata": {},
   "source": [
    "Ok, so now we have seeded and crawled our database with some receipts data.  If you look at the `etl/tx_drinks/run.py` file, you can see that our flow works similarly, but slightly differently from what we just walked through.\n",
    "\n",
    "The main issue is that when we have a new restaurant we want to pull all data, and when we already have restaurant data in S3, we just want to pull in only the new data.\n",
    "\n",
    "* find_and_coerce\n",
    "    * This calls `receipts_client.find`, which first looks for new data, or if we don't have receipts for the restaurant in the db will find all data. \n",
    "   * `receipts_client.find_recent()`\n",
    "        * `queries.find_last_end_date(name)`\n",
    "            * It does this by first finding the last obligation end_date of the specified restaurant.  \n",
    "        * `receipts_client.find_receipts_after(name, last_end_date)`\n",
    "            * Then having been provided that end date, we query the api only for restaurant receipts after that date.\n",
    "        * `find_all_receipts()`\n",
    "            * Called if there are no existing receipts. \n",
    "\n",
    "* `find_and_coerce_llcs`()\n",
    "    * calls `find_and_coerce` for all llcs stored in our s3 file.\n",
    "    * From there, we coerce the dataframe, and store in s3.\n",
    "\n",
    "So the above approach will allow us to just query for new receipts that we have not seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1c096-7bea-4bbc-b7a9-659dd45efb4b",
   "metadata": {},
   "source": [
    "### Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c478c6-9987-40e8-94f5-70c0e7ae2469",
   "metadata": {},
   "source": [
    "Now onto the `flow.py` file.  Notice that once again there is almost **hardly any code** in the flow.py file.\n",
    "\n",
    "The only exception is the `find_and_write_receipts` flow.  And even that, we have another function that in run.py that essentially does the same thing.  \n",
    "\n",
    "So again, the point is to test and write as much logic as possible outside of our workflow manager, prefect.  This makes it easier to test, and rapidly speeds up the feedback loop when we try our code, instead of the slow task of waiting for a flow to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b07336-69fb-4ed3-bb8c-ad62d462ea9c",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "So think about the steps we needed to perform.\n",
    "\n",
    "A. Call data and store in s3\n",
    "\n",
    "1. Call an API\n",
    "2. Store the results in an s3\n",
    "3. But first, make sure to coerce the data to make it as much as possible into a numeric format\n",
    "\n",
    "B. Setup glue\n",
    "\n",
    "1. Create the database\n",
    "2. Crawl the bucket to create a new table\n",
    "3. Try querying the table through athena\n",
    "\n",
    "C. Only store new data\n",
    "\n",
    "1. Use athena to find the last date for a given restaurant\n",
    "2. Then query the api only for information after that date\n",
    "3. Then loop through for all restaurants\n",
    "\n",
    "D. Integrate prefect\n",
    "\n",
    "* Finally create a flow.py file that will for each restaurant\n",
    "    * find_and_coerce the recent data\n",
    "    * write the data to s3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
