{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8210ee6f-4522-4272-bd66-d98d0faafa34",
   "metadata": {},
   "source": [
    "# Wrangler Prefect Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3cf33-0715-4e29-a90f-02fcb6b735cc",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7328009-a6d6-445b-9361-3b09a1e04fb0",
   "metadata": {},
   "source": [
    "In this lesson, we'll move through the wrangler prefect lab.  The lab will use the texas drink receipts api. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce33d3-3785-4fe8-b0be-b30d4acba6a5",
   "metadata": {},
   "source": [
    "### Project Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce7172-c1cc-4176-b3e9-af51bb067a2c",
   "metadata": {},
   "source": [
    "Before moving on, let's talk through why this topic can make for a good project.  We can use alcoholic receipts as a proxy for revenue of a company, and here we can get insights on a per store basis, that we cannot get from a company report.  \n",
    "\n",
    "For example, in the `etl/tx_drinks/seed` file, you can see that we chose restaurants that are publicly listed.  The last set of restaurants are different Applebees chains.  We could imagine a project showing the following.\n",
    "\n",
    "* Overview analysis\n",
    "1. Showing a relationship between drink revenue and overall revenue\n",
    "2. Master/map display of revenue per store\n",
    "3. Detail - data about an individual store, as we click on a store\n",
    "    * Eg. Revenue per month, monthly sales in previous year vs current\n",
    "    \n",
    "* Deeper Analysis\n",
    "And then if given more time, we could explore location based metrics.\n",
    "1. Does income of region predict revenue?\n",
    "2. How about population size?\n",
    "3. Or sales of nearby competitors (eg. Chilis)\n",
    "4. Outliers - Are there over or underperformers that we should look more deeply at?\n",
    "\n",
    "Stats like this would allow us to (1) justify where to open/close retail locations of similar stores (2) assess performance to dig deeper and learn about a store's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b381e-d781-4963-ae22-410d59cf1bc3",
   "metadata": {},
   "source": [
    "### Getting setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127b569-9b92-484e-b8dc-be6b72595cf2",
   "metadata": {},
   "source": [
    "To be able to run the code, we should setup our virtual environment, and install the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375fe28c-20e3-47a9-82ab-dfb38a037eb3",
   "metadata": {},
   "source": [
    "```bash\n",
    "python3 python -m venv ./venv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578d67df-f433-4bd1-836c-38e1190f19c2",
   "metadata": {},
   "source": [
    "```bash\n",
    "source venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11d87d-af4a-48ca-9572-2929aa1f7b84",
   "metadata": {},
   "source": [
    "Now that the environment is activated, we can install the packages in the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad06cf-8e5c-47ab-ada2-63cc48123ec2",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip3 install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582345ab-2dc7-4856-a2df-e0f3d9b34e50",
   "metadata": {},
   "source": [
    "And then we should assign the `PYTHONPATH` to the path of our `revenue_tracker` folder.  This will mean that as we import files, it will always relative to the specified revenue_tracker.  \n",
    "\n",
    "In my environment, I set this by placing the following into bash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d84771-7422-4e3d-ba43-2e3629c4c733",
   "metadata": {},
   "source": [
    "```bash\n",
    "export PYTHONPATH=/Users/jeffreykatz/Documents/jigsaw/curriculum/1-career-services/prefect-lessons/9-aws-wrangler-lab/revenue_tracker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92447cbb-df60-4b14-b126-312c76899892",
   "metadata": {},
   "source": [
    "But you should navigate to your `revenue_tracker` folder, and type `pwd` to find the absolute path to your folder.  \n",
    "\n",
    "Now, when in our code we have something like:\n",
    "\n",
    "```python\n",
    "import settings\n",
    "```\n",
    "\n",
    "It will look for the module in the `revenue_tracker` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74fbaef-8104-43f1-b082-fd6850a6a1eb",
   "metadata": {},
   "source": [
    "From there, we should set the environmental variables, which you can see in the `.env` file.  As you can see, we need a specified bucket folder to store our data, as well as a name for our glue_db (eg. `revenue_tracker` is fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686617d-014f-4fad-8a0b-9ac9e0b42aa4",
   "metadata": {},
   "source": [
    "### Kicking off the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2aca0-f96f-4b57-80a1-afbfee07dd73",
   "metadata": {},
   "source": [
    "A good place to start with the project is the `console.py` file.  At the bottom, you can find some useful functions to try out.\n",
    "* `find_all_receipts`\n",
    "    * `receipts_client.find_all_receipts`\n",
    "        * Our client pulling down data from the api\n",
    "    * `receipts_adapter.coerce_df`\n",
    "        * This function is tricky.  The idea is to use pandas to convert as many columns as possible into either a numeric or datetime column.  This way, when we save the dataframe to athena, it will have those numeric/datetime datatypes.  Notice that we return if the dataframe is empty, which may occur if we do not get records back from the api.\n",
    "        \n",
    "* `aws_utils.write_to_s3`\n",
    "    * this is in the `aws_utils` as there are multiple times that we may want to write to s3, and we wanted to avoid repeating the code.\n",
    "    \n",
    "> Choosing the partition key\n",
    "\n",
    "> To choose the partition, we considered various candidates.  Using an item like obligation_end_date was the first consideration (and what we went with) because it consistently updated each month.  Still, we coerced it into a date -- and perhaps should have only selected month and year -- as too detailed (eg. considering the time, or day) could lead to partition explosion.  We also considered using additional partition keys of say restaurant name (which could make when storing data from multiple restaurants), however we saw there were only 21 locations per an example restaurant, which seemed like to sparse a partition.\n",
    "\n",
    "```python\n",
    "df.location_address.unique()\n",
    "# 21\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8283ad5-d36c-4066-b026-f10c20b3ac2e",
   "metadata": {},
   "source": [
    "So that's the first major step, `find_and_coerce` the data, and then write it to s3.  One thing to note is that by default, the api only returns 1000 records.  So to seed our database, we would [change the limit](https://dev.socrata.com/docs/paging.html) to the max of `50_000` records.  This would give us an initial dataset, and we could schedule our prefect code to repeatedly update this (more below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b689a308-6980-4ed5-9f0b-5819f35929e6",
   "metadata": {},
   "source": [
    "### Setting up Glue/Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37845d9-d172-4c61-b26f-779b9dbc7151",
   "metadata": {},
   "source": [
    "So after we have seeded our data lake with data, the next step is to tell Glue to crawl it so that we can query the data with a query engine like Athena or Pyspark.\n",
    "\n",
    "For this, we can continue on at the bottom of the `console.py` file.  \n",
    "\n",
    "* `athena.migrations.create_db`\n",
    "    * This creates a database in glue.\n",
    "    \n",
    "* `athena.migrations.crawl_dataset(\"receipts\")`\n",
    "    * This creates a new table in our database called `receipts`, sets the specified datatypes, and seeds with our data.\n",
    "    * One confusing component, is that because we partitioned by `obligation_end_date`, this will be an object, and not our preferred datetime.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c97a5-fd3f-4bd5-af38-d78f21bfd1ce",
   "metadata": {},
   "source": [
    "Still if we run those two functions, we should see our database seeded with some initial data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a01e3-7985-4752-b3cf-dfd9b42c73d9",
   "metadata": {},
   "source": [
    "* `athena.queries.read_query`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702fe6ec-7cde-459b-ba17-40230e050a7c",
   "metadata": {},
   "source": [
    "From there, we can query our dataset with athena with our `read_query(query)` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a558b-db88-4b2e-aebb-5fa554f8db67",
   "metadata": {},
   "source": [
    "### Seeding a list of llcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2fff2-7647-451b-b38b-170d08a57516",
   "metadata": {},
   "source": [
    "Eventually, we would like this to work for a list of llcs.  And we want this to be a common list of llcs regardless of whose computer is running this.\n",
    "\n",
    "So in the console.py file you can see that we call `write_llcs`, which writes to a parquet file in S3, a list of llc names defined in `seed/restaurant_llcs.py`.  Then we can read them and get back a list of llcs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4577f-8c2b-43de-8ded-e1513c6f3128",
   "metadata": {},
   "source": [
    "### Our Run file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2ab12-5f62-414b-8b4b-991fd067154e",
   "metadata": {},
   "source": [
    "Ok, so now we have seeded and crawled our database with some receipts data.  If you look at the `etl/tx_drinks/run.py` file, you can see that our flow works similarly, but slightly differently from what we just walked through.\n",
    "\n",
    "The main issue is that when we already have restaurant data in S3, we just want to pull new data, and when it's a new restaurant we want to pull all data.\n",
    "\n",
    "* find_and_coerce\n",
    "    * This calls `receipts_client.find`, which first looks for new data, or if we don't have receipts for the restaurant in the db will find all data. \n",
    "   * `receipts_client.find_recent()`\n",
    "        * `queries.find_last_end_date(name)`\n",
    "            * It does this by first finding the last obligation end_date of the specified restaurant.  \n",
    "        * `receipts_client.find_receipts_after(name, last_end_date)`\n",
    "            * Then having been provided that end date, we query the api only for restaurant receipts after that date.\n",
    "        * `find_all_receipts()`\n",
    "            * Called if there are no existing receipts. \n",
    "\n",
    "* `find_and_coerce_llcs`()\n",
    "    * calls find_and_coerce for all llcs stored in our s3 file.\n",
    "    * From there, we coerce the dataframe, and store in s3.\n",
    "\n",
    "So the above approach will allow us to just query for new receipts that we have not seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1c096-7bea-4bbc-b7a9-659dd45efb4b",
   "metadata": {},
   "source": [
    "### Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c478c6-9987-40e8-94f5-70c0e7ae2469",
   "metadata": {},
   "source": [
    "Now onto the flow.py file.  Notice that once again there is almost **hardly any code** in the flow.py file.\n",
    "\n",
    "The only exception is the `find_and_write_receipts` flow.  And even that, we have another function that in run.py that essentially does the same thing.  \n",
    "\n",
    "So again, the point is to test and write as much logic as possible outside of our workflow manager, prefect.  This makes it easier to test, and rapidly speeds up the feedback loop when we try our code, instead of the slow task of waiting for a flow to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
